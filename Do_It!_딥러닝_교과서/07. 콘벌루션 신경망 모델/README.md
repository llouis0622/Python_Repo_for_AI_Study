# 07장. 콘벌루션 신경망 모델

# 1. 르넷-5

- 우편물에 필기체로 쓰인 우편번호를 인식하는 최초의 콘벌루션 신경망

## 1. 모델 구조

- CONV-POOL-CONV-POOL-FC-FC-FC
- 콘벌루션 계층 + 풀링 계층 2번 반복, 완전 연결 3계층
- 콘벌루션 계층, 풀링 계층 : 생체 신경망의 시각 영역
- 완전 연결 계층 : 연관 영역
- 활성 함수 : 시그모이드, 하이퍼볼릭 탄젠트

# 2. 알렉스넷(AlexNet)

- 이미지넷(ImageNet) : 약 1400만 개의 이미지로 구성된 세계에서 가장 방대한 이미지 데이터 베이스, 2만여 개의 카테고리 포함

## 1. 모델 구조

- 2개의 GPU에서 실행되는 병렬처리 구조
- 콘벌루션 필터를 두 그룹으로 나눈 뒤 그룹별로 GPU 할당 후 처리, 중간 계층에서 정보 교환 → 최종 결과를 한쪽 그룹으로 합침
- CONV-CONV-CONV-CONV-CONV-FC-FC-FC
- 콘벌루션 5계층 + 완전 연결 3계층
- LRN(Local Response Normalization) : 지역 응답 정규화, 픽셀별로 이웃 채널의 픽셀을 이용하여 정규화

## 2. 훈련 방식

- 정규화 기법 : LRN, 드롭아웃, $L_2$ 정규화, 데이터 증강
- 최적화 알고리즘 : SGD 모멘텀
- 배치 크기 : 128

# 3. 제트에프넷(ZFNet)

- ILSVRC 2013 대회에서 우승한 모델, 모델 시각화 방식

## 1. 모델 구조

- CONV1 콘벌루션 11×11, 4 → 7×7, 2
- CONV3, CONV4, CONV5 필터 개수 : 384, 384, 256 → 512, 1024, 512
- GPU 1개 사용

## 2. 모델 시각화 방식

- 콘벌루션 신경망을 시각화 → 모델의 학습 과정과 학습 내용 이해 가능
- 계층이 높아질수록 수용 영역 넓어짐, 지역적 특징에서 전역적 특징으로 학습 내용 변함
- 낮은 계층에서 먼저 학습되기 시작해서 점점 높은 계층으로 학습 진행
- 입력에 작은 변화가 있어도 동일한 결과를 출력하는 위치불변성 가짐
- 특징을 포착할 때 모양 및 위치까지 가늠
- 디콘벌루션 네트워크(Deconvolutional Network) : 콘벌루션 연산의 역연산

# 4. 브이지지넷(VGGNet)

- 작은 필터를 사용하는 대신 깊은 신경망을 만들자
- 3×3 콘벌루션 필터만 사용

## 1. 설계 사상

- 신경망 깊이 증가 →  수용 영역 넓어짐 → 필터 작게 만듦 →  파라미터 수 감소, 신경망 깊이 증가

## 2. 모델 구조

- 11계층, 13계층, 16계층, 19계층
- VGG16 : Conv1-Conv2-Conv3-Conv4-Conv5-FC
- 이미지 특징 추출

# 5. 구글넷(GoogleNet)

- ILSVRC 2014 대회에서 우승한 모델
- 네트워크 속 네트워크(NIN, Network In Network) : 네트워크 모듈을 쌓는 방식
- 인셉션(Inception) : 네트워크 모듈

## 1. 설계 사상

- 희소 성질(Sparsity) : 극히 일부 구성 요소 사이에서만 상호 작용이 일어남
- 조밀 성질(Density) : 대부분의 구성 요소 사이에 상호 작용이 일어남
- 인셉션 모듈 : 희소 연결 구조 + 조밀 연산
- 1×1 콘벌루션 : 채널 특징 인식
- 3×3 콘벌루션, 5×5 콘벌루션 : 서로 다른 크기의 수용 영역에서 공간 특징과 채널 특징을 통합적으로 인식
- 맥스 풀링 : 가장 두드러진 특징 인식

## 2. 인셉션 모듈

- 기본 인셉션 모듈(Naïve Inception Module) : 역할 분리, 희소 연결, 조밀 연산이라는 인셉션 모듈의 설계 사상에 따라 정의
- 개선된 인셉션 모듈 : 병목 계층을 두어 채널 수와 연산량 대폭 감소

## 3. 모델 구조

- 스템(Stem) : 신경망 도입부, 콘벌루션과 풀링으로 구성
    - CONV-POOL-CONV-CONV-POOL
- 몸체(Body) : 인셉션 모듈을 9개 쌓은 구조
    - 1, 3, 8번째 인셉션 모듈 전 맥스 풀링을 둠 → 액티베이션 맵 크기 감소
- 최종 분류기(Final Classifier) : 완전 연결 계층 → 평균 풀링 → 파라미터 수 대폭 감소
    - AvgPOOL-FC-Softmax
- 보조 분류기(Auxiliary Classifier) : 2개의 보조 분류기
    - 하위 계층에 그레이디언트를 원활히 공급, 모델 정규화
    - 훈련 용도
    - AvgPOOL-1×1CONV-FC-FC-Softmax

# 6. 레즈넷(ResNet)

- ILSVRC 2015 대회 & COCO 대회의 우승 모델
- 152계층의 깊은 신경망
- 잔차 연결(Residual Connection)
- 레즈 블록(Residual Block) : 네트워크 모듈

## 1. 설계 사상

- 얕은 모델을 낮은 계층에 복사, 위에 항등 매핑 계층을 쌓음
- 레즈 블록 : 콘별루션 계층이 포함된 학습 경로, 항등 함수로 정의된 항등 매핑 경로로 구성
- 잔차 연결 : 레즈 블록의 항등 매핑
- 잔차 경로(Residual Path) : 학습 경로에서 잔차 학습
    
    $$
    F(x)=H(x)-x
    $$
    

## 2. 모델 구조

- 스템 : 신경망 도입부, 7×7 콘벌루션 계층
- 몸체 : 레즈 블록을 쌓은 부분, 모델 구성에 따라 달라짐
- 분류기 : 전역 평균 풀링(Global Average Pooling) → 파라미터 수 대폭 감소
    - GlobalAvgPOOL-FC-Softmax

## 3. 훈련 방식

- 기본 정규화 기법 : 배치 정규화
- $L_2$ 가중치 감소, 데이터 확장
- 활성 함수 : ReLU
- 최적화 알고리즘 : SGD 모멘텀
- 배치 크기 : 256

# 7. 콘벌루션 신경망 비교

![1.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81ce352b-f505-4b7c-ba55-561d76267295/9527dd9a-8c35-4cf9-abb8-45bb95b52e1f/1.png)

![1.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/81ce352b-f505-4b7c-ba55-561d76267295/2aab372a-a589-4c12-bb49-2b895cd358b5/1.png)

# 8. 다양한 모델의 등장

## 1. 레즈넷 개선 모델

### 1. 레즈 블록의 연산 순서와 항등 매핑 개선

- 가중 합산 후 활성화 순서 → 활성화 후 가중 합산 순서
- BN-ReLU-CONV-BN-ReLU-CONV

### 2. 와이드 레즈넷(Wide Residual Networks)

- 신경망의 너비를 늘리는 구조로 설계
- 레즈 블록이 병렬 처리될 수 있도록 넓은 레즈 블록으로 확장해서 계산 효율 증가

### 3. 레즈넥스트(ResNeXt)

- 레즈 블록의 잔차 경로를 여러 개로 분리해서 병렬 실행
- 분리-변환-통합 전략 + 동일한 블록 반복 → 하이퍼파라미터 수 감소

### 4. 깊이가 확률적으로 변하는 신경망(Deep Networks with Stochastic Depth)

- 훈련하는 동안 네트워크를 짧게 만들어서 그레이디언트 소실을 줄여보자
- 훈련 시 부분 경로 드롭아웃, 테스트 시 전체 경로 사용

## 2. 레즈넷을 넘어선 모델

### 1. 프랙탈넷(FractalNet)

- 잔차 구조보다는 얕은 네트워크에서 깊은 네트워크로의 효율적인 전환이 성능에 더 중요함
- 얕은 경로 + 깊은 경로 모두 가짐
- 훈련 시 부분 경로 드롭아웃, 테스트 시 전체 경로 사용

### 2. 댄스넷(DenseNet)

- 댄스 블록으로 구성
- 모든 계층이 이전의 모든 계층과 연결된 구조
- 이전 계층의 출력을 채널 방향으로 합침

### 3. 나스넷(NASNet)

- 하위 계층이 대부분의 상위 계층과 연결
- 첫 번째 계층이 거의 모든 계층과 연결

## 3. 인셉션 개선 모델

- 인셉션 모듈 A, B, C : 35×35, 17×17, 8×8 액티베이션 맵
- 리덕션 모듈 A, B : 35×35 이미지 → 17×17 이미지, 17×17 이미지 → 8×8 이미지

### 1. 인셉션 모델에 잔차 연결을 추가한 인셉션-레즈넷

- 잔차 연결과 함께 간소화된 인셉션 연산 분기
- 분기를 합치기 전 차원을 맞추기 위해 1×1 콘벌루션 사용
