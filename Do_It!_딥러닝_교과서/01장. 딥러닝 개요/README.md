# 01장. 딥러닝 개요

# 1. 딥러닝이란?

## 1. 인공지능, 머신러닝과 딥러닝 정의

- 인공지능(AI, Artificial Intelligence) : 문제를 인식하고 해결하는 능력인 지능을 구현하는 기술
- 머신러닝(ML, Machine Learning) : 기계 스스로 학습하여 지능을 습득하는 기술
- 딥러닝(DL, Deep Learning) : 생체 신경망을 모방해서 만든 인공 신경망(ANN)을 이용하여 복잡한 데이터 관계를 찾아내는 머신러닝 기법

## 2. 머신러닝과 딥러닝의 관계

### 1. 머신러닝 기법 구분

- 지도 학습(Supervised Learning) : 입력 데이터에 대한 정답을 사전에 정의해서 학습 데이터로 제공하는 것, 자신의 예측과 정답의 차이를 보고 오차를 줄임
    - 분류(Classification) : 데이터를 클래스별로 분류
    - 회귀(Regression) : 데이터의 함수적 관계를 알아냄
    - 모델(Model) : 다양한 변수 간 수학적/확률적 관계를 표현하는 자료구조, 알고리즘, 프로그램
    - 타깃(Target), 레이블(Label) : 학습 데이터를 통해 제공하는 입력 데이터의 정답
- 비지도 학습(Unsupervised Learning) : 전문가의 개입 없이 순수하게 데이터만으로 학습하는 방법
    - 클러스터링(Clustering) : 비슷한 데이터끼리 묶어줌
    - 차원 축소(Dimension Reduction) : 고차원 데이터를 저차원 데이터로 변환
    - 표현 학습(Representation Learning) : 데이터의 핵심적인 정보를 낮은 차원의 잠재 데이터로 표현
    - 데이터 생성(Data Generation) : 새로운 데이터 생성
    - 연관 규칙(Association Rule) : 데이터 간의 규칙을 찾아냄
    - 협업 필터링(Collaborative Filtering) : 사람의 행동을 분석하여 관심 항목 추천
- 강화 학습(Reinforcement Learning) : 순차적인 의사 결정 문제(Sequential Decision Problem)

### 2. 머신러닝에 전반적으로 적용되는 딥러닝

- 심층 비지도 학습(Deep Unsupervised Learning), 심층 강화 학습(Deep Reinforcement Learning)

## 3. 딥러닝의 장점과 한계

### 1. 딥러닝의 장점

- 함수를 근사하는 능력이 뛰어남
- 특징을 자동으로 추출
- 모델의 확장성이 뛰어남
- 기존 머신러닝보다 훨씬 좋은 성능을 보임

### 2. 딥러닝의 한계

- 파라미터(Parameter)가 많기 때문에 다른 머신러닝 모델보다 상대적으로 많은 학습 데이터 필요
- 훈련을 위한 시간과 비용이 많이 듦
    - 전이 학습(Transfer Learning) : 대용량 데이터로 사전에 학습된 모델을 이용해서 빠르게 학습
    - 메타 학습(Meta Learning) : 여러 작업의 학습 방식을 학습한 후에 유사한 작업을 적은 데이터로 빠르게 학습
    - 평생 학습(Lifelong Learning) : 한 번 학습한 내용을 잊지 않고 계속해서 새로운 것을 누적해서 학습해 나감
    - 액티브 학습(Active Learning) : 성능에 큰 영향을 미치는 데이터를 선별하여 학습함으로써 선별된 적은 데이터로 빠르게 학습
- 설정 파라미터가 많아서 최적의 모델과 훈련 방법을 찾으려면 상당히 많은 검색 시간과 튜닝 시간이 필요
- 인공 신경망 모델은 오류를 파악하거나 디버깅하기 어려움
- 지도 학습에서는 타깃 데이터를 만들 때 드는 비용이 만만치 않음
    - 생성 모델(Generative Model)을 이용하여 훈련 데이터를 자동으로 생성
    - 준 지도 학습(Semi-Supervised Learning) : 사람이 제공하는 힌트 데이터나 타깃 데이터를 이용하여 자동으로 타깃을 만듦
    - 자기 지도 학습(Self-Supervised Learning) : 학습 과정에 사람이 개입하지 않아도 됨

# 2. 인공 신경망의 탄생

## 1. 지능을 만들고 싶은 인간

- 지능(Intelligence) : 어떤 문제에 당면했을 때 자신의 지식과 경험을 활용해 문제를 해결하는 능력

## 2. 뇌, 신경망 그리고 지능

- 뉴런주의(Neuron Doctrine) : 뇌의 기능 단위는 뉴런이다

# 3. 딥러닝의 역사

## 1. 최초의 인공 신경망 : 매컬러-피츠 모델(McCulloch-Pitts)

- 최초의 인공 신경망 - 인간의 신경계를 이진 뉴런(Binary Neuron)으로 표현
    - 활성 상태와 비활성 상태를 가짐

## 2. 학습하는 인공 신경망 : 퍼셉트론(Perceptron)

- 인공 신경망이 스스로 문제에 맞춰 학습하는 모델
- 새로운 입력에 대한 오차가 발생하면 뉴런의 연결 강도를 조절

### 1. 퍼셉트론의 구조

- 입력 데이터가 들어오면 가중치와 곱해서 가중 합산을 하며 그 결과가 0보다 크면 1을 출력하고 그렇지 않으면 0을 출력
- 활성 함수(Activation Function) : 계단 함수(Step Function)
- 선형 분류기(Linear Classifier) : 두 종류의 클래스를 직선으로 분류
- 결정 경계(Decision Boundary) : 입력 데이터와 가중치의 가중 합산 식은 두 클래스를 분류함

### 2. 생체 신경망을 모방하여 만든 퍼셉트론

- 입력 $x^T = (x_1, x_2, ..., x_n)$ : 이전 뉴런이 발화한 신호
- 가중치 $w^T = (w_1, w_2, …, w_n)$ : 시냅스의 연결 강도
- 입력 데이토와 가중치의 곱 $w_i x_i(i = 1, 2, …, n)$ : 시냅스의 연결 강도에 따라 신호가 강해지거나 약해지는 과정
- 가중 합산 $z = \sum_{i=1}^{n} {w_i x_i} + b$ : 세포체에서 수상 돌기를 통해 들어온 신호를 모으는 과정
- 활성 함수 $f(z)=\begin{cases} 1 \ \text{if} \ z≥ 0 \\ 0 \ \text{그 \ 외의 \ 경우} \end{cases}$ : 세포체의 신호 발화 과정
- 출력 $f(z) = f(w^Tx + b)$ : 축삭을 따라 시냅스로 전달되는 과정

## 3. 신경망 연구의 암흑기를 불러온 책 <퍼셉트론>

- 기호주의(Symbolism) : 실세계의 사물과 사상을 기호화하고 그들 사이에 관계를 정해주면 논리적 추론을 통해 지능을 만들 수 있음
- 연결주의(Connectionism) : 뉴런 수준에서 지능이 형성되는 과정을 모방하면 데이터로부터 스스로 지능을 만들 수 있음

### 1. XOR 논리 연산으로 제기된 다층 퍼셉트론의 필요성

- 두 직선을 정의하는 첫 번째 계층과 AND 연산을 하는 두 번째 계층으로 구성되는 인공 신경망을 정의해야 이 문제를 풀 수 있는데 퍼셉트론은 하나의 계층만 표현할 수 있기 때문에 문제를 풀 수 없음

## 4. 역전파 알고리즘의 발견

- 역전파(Backpropagation) 알고리즘 : 다층 퍼셉트론을 학습시킬 수 있는 것

### 1. 퍼셉트론의 한계를 극복한 역전파 알고리즘

- 인공 신경망은 학습 과정에서 출력과 정답의 오차를 최소화하도록 최적화(Optimization) 수행
- 역전파 알고리즘 : 신경망의 뉴런에 분산된 파라미터의 미분을 효율적으로 계산하기 위한 알고리즘
    - 출력 계층(Output Layer) → 입력 계층(Input Layer) 방향으로 한 계층씩 이동하며 미분

## 5. 딥러닝 시대를 열다

### 1. 모델이 데이터를 암기하는 현상 : 과적합

- 적합(Good Fitting) : 모델이 관측 데이터를 잘 설명하는 곡선을 표현하는 것
- 과적합(Overfitting) : 모델이 모든 점을 지나는 복잡한 모양의 곡선을 표현하는 것
    - 모델이 과도하게 학습되어 데이터를 암기한 상태
- 과소적합(Underfitting) : 모델이 표현하는 곡선이 데이터의 분포를 제대로 반영하지 못하는 것

### 2. 학습이 중단되는 현상 : 그레이디언트 소실

- 그레이디언트 소실(Gradient Vanishing) : 깊은 신경망을 학습할 때 역전파 과정에서 미분값이 사라지면서 학습이 중단되는 현상

### 3. 깊은 신경망을 안정적으로 학습시킬 수 있는 딥러닝

- 제한적 볼츠만 머신(RBM, Restricted Boltzmann Machine) : 사전 학습한 뒤에 한 계층씩 쌓아서 깊은 신경망을 만드는 방식
    - 입력 계층과 은닉 계층(Hidden Layer)으로 이루어진 에너지 기반의 생성 모델
